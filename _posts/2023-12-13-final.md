---
layout: post
title:  "Scraping PSA Card Website"
author: Thomas Olsen
description: Using BeautifulSoup to webscrape
image: /assets/images/baseballCards.jpg
---

## Background of Sport Card Grading
Sports card collecting is a hobby enjoyed by many around the world. People like to collect cards from their favorite players and team.  There is also a lively market for buying and selling sports cards.  One of the factors that determines the value of the card is the condition that it is in. Companies like the Professional Sports Authenticator (PSA) give the cards a grade on a scale from Authentic (i.e. 0 but real) to 10 (Gem Mint).  PSA keeps track of the cards that they have graded in a table on their website in a section called the pop report.  

## Introduction: Motivation and Considerations
The goal in scraping the PSA [website](https://www.psacard.com/pop/baseball-cards/20003) pop report is to explore the overall trends in baseball card history.  I am an avid baseball fan as well as a sports card collector.  However the world of sports cards is very large and complicated, so this project's goal is to try to understand the trends of the hobby.

Before I started this project I consulted the [www.psacard.com/robots.txt](https://www.psacard.com/robots.txt).  The only request that it made was to add a crawl-delay of 1, but we are free to scrape the pop report.

The link to the github repository can be found [here](https://github.com/tolsen25/PSA_Pop_Project.git)
***
## Part 1: Exploring the tables
The years with the blue underline are links to tables.

![img1](/assets/images/links.png "links")

These links leads to a table that looks like the following for the rest of the years.  

![img2](/assets/images/setImages.png "links")

Thus instead of scraping each individual table we can scrape all of them at once using BeautifulSoup
Of note the robots.txt requests crawl-delay of one so when we scrape all of the sets we will implement that delay.

***
## Part 2: Scraping the links
Inspecting the HTML reveals that the data we are looking for actually starts on the third <tr> tag.  However since baseball was not around until 1861 I decided to start the loop on the fourth <tr> tag so that the data would start from the year 1861.  Once the links are scraped they are ready to be passed through a second webscraper that will scrape the pop counts
```
import requests
from bs4 import BeautifulSoup
import pandas as pd

# URL of the psa.com website with the links we want to scrape
url = 'https://www.psacard.com/pop/baseball-cards/20003'

r = requests.get(url)
bs = BeautifulSoup(r.text, "html.parser")

# Send an HTTP request to the URL
response = requests.get(url)

tableRows = bs.findAll('tr')

links = []
for row in tableRows[3:]: # start at the third tr
  firstTd = row.find("td")
  setUrl_text = firstTd.find("a")
  setUrl = setUrl_text.get('href')
  links.append(setUrl)
  #print(setUrl_text)

df2 = pd.DataFrame(links)
df2["links"] = "https://www.psacard.com"  + df2[0]

df2 = df2.iloc[:,1:]
df2.to_csv("setLinks.csv", index = False)

```
***
## Part 3: Scraping the pop count
Scraping the pop count itself took two webscrapers itself.  The first grabbed the headers of the file which became the columns of the dataframe.
```
import pandas as pd
import requests
from bs4 import BeautifulSoup
import time

urlTable = pd.read_csv("setLinks.csv")
pattern = r'/(\d{4}(?:-\d{2,4})?)/'

# Use str.extract to create a new 'Year' column
urlTable['Year'] = urlTable['links'].str.extract(pattern)

urls = urlTable["links"]

url = "https://www.psacard.com/pop/baseball-cards/2023/221547" # Test the connection
r = requests.get(url)

r.ok
len(r.text)

bs = BeautifulSoup(r.text, "html.parser")

# The first <tr> tag has the header information that we need
cardHeader = bs.find('tr', {'class' : 'text-uppercase'})

headers = []
for i in cardHeader.find_all('th'):
 title = i.text
 headers.append(title)
 
df = pd.DataFrame(columns = headers)
df = df.drop(df.columns[[0,2]], axis =1)
df.insert(1, "Year","")

```

Then that dataframe was passed on to the second scraper.  This second scraper loops through all of the urls and adds each set and all of their grades from every year to the dataframe row by row.

```
sets=[]
testList=[]
#for rowNum, url in enumerate(urls):
for url in urls:
   # reset the list
  r = requests.get(url)
  if r.ok:
    bs = BeautifulSoup(r.text, "html.parser")
    time.sleep(3)
    cardRows = bs.find_all('tr') 
    Year = pd.Series(url).str.extract(pattern).iloc[0,0] # grabs the year from the url
    print(Year)
    #print("entering second for loop")
    for row in cardRows[2:]:
      testList = []
      set_TD = row.find("td", {"class": "text-left"})
      if set_TD:
        setName = set_TD.find("a").text
    
        sets.append(setName) # append set
        testList.append(setName)
        all_td = row.find_all("td")
     
        testList.append(Year) # append year
          #print(all_td)
        for td in all_td[3:]: 
          Grade = td.find("div").text
          testList.append(Grade) # append the grades
      
      newRow = pd.DataFrame([testList], columns = df.columns)
      df = pd.concat([df,newRow], ignore_index = True)
      else:
        print("Set name not found in row:", idx)
        #break
    print("Processed:", url)
```
Finally the data is saved as a pickle so that it can be loaded quickly for data wrangling
```
df.to_pickle("allSets_final.pkl")
```

***
## Part 4: Data wrangling
Some obvious issues with the data were that some sets were across multiple years (i.e. 1961-1965). To deal with this I simply used the first 4 digits as the year in essence treating the year as the when it first came out.  I then tidyed it by pivoting the data and cleaning the `Count` column so that it was numeric.  In the end my dataset went from

![img5](/assets/images/wide.png "links")

to 

![img5](/assets/images/tidy.png "links")

## Conclusion:
Scraping this set using BeautifulSoup required learning pandas dataframe concatenation and careful attention to the html structure of the webpage.  Improvements could be made to add try-catch functionality to prevent the scraper from getting stuck if one of the webpages does not work correctly.



