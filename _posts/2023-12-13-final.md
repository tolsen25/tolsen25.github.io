---
layout: post
title:  "Scraping PSA Card Website"
author: Thomas Olsen
description: Using BeautifulSoup to webscrape
image: /assets/images/baseballCards.jpg
---

## Background of Sport Card Grading
Sports card collecting is a hobby enjoyed by many around the world. People like to collect cards from their favorite players and team.  There is also a lively market for buying and selling sports cards.  One of the factors that determines the value of the card is the condition that it is in. Companies like the Professional Sports Authenticator (PSA) give the cards a grade on a scale from Authentic (i.e. 0 but real) to 10 (Gem Mint).  PSA keeps track of the cards that they have graded in a table on their website in a section called the pop report.  

## Part 1
#### Exploring the tables
The years with the blue underline are links to tables.  
![img1](/assets/images/links.png "links")
These links leads to a table that looks like the following for the rest of the years.  
![img2](/assets/images/setImages.png "links")
Thus instead of scraping each individual table we can scrape all of them at once using BeautifulSoup
Of note the robots.txt requests crawl-delay of one so when we scrape all of the sets we will implement that delay.

## Part 2
#### Scraping the links
Inspecting the HTML reveals that the data we are looking for actually starts on the third <tr> tag.  However since baseball was not around until 1861 I decided to start the loop on the fourth <tr> tag so that the data would start from the year 1861.  Once the links are scraped they are ready to be passed through a second webscraper that will scrape the pop counts
![img3](/assets/images/scrapeLinks.png "links")

## Part 3
#### Scraping the pop count
Scraping the pop count itself took two webscrapers itself.  The first grabbed the headers of the file which became the columns of the dataframe.
![img4](/assets/images/cardHeaders.png "links")
Then that dataframe was passed on to the second scraper.  This second scraper loops throug all of the urls and adds each set and all of their grades from every year to the dataframe row by row.
![img5](/assets/images/fullScrape.png "links")
Finally the data is saved as a pickle so that it can be loaded quickly for data wrangling

## Part 4
#### Data wrangling
Some obvious issues with the data were that some sets were across multiple years (i.e. 1961-1965). To deal with this I simply used the first 4 digits as the year in essence treating the year as the when it first came out.  I then tidyed it by pivoting the data and cleaning the `Count` column so that it was numeric.  In the end my dataset went from
![img5](/assets/images/wide.png "links")
to 
![img5](/assets/images/tidy.png "links")

